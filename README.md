# ğŸ¤– Vision-Based Gesture-Controlled Robot using Deep Reinforcement Learning

An end-to-end robotic control system built from scratch â€” integrating computer vision, deep learning, and motor control for real-time gesture-driven navigation.

----
### ğŸ§  Overview

This project implements a gesture-controlled mobile robot using computer vision (OpenCV + MediaPipe) and deep neural gesture recognition models (TensorFlow/Keras).
The robot interprets human hand gestures in real time to perform navigation tasks like move forward, reverse, turn left/right, and stop, leveraging a Raspberry Pi for onboard computation and motor control via an H-Bridge driver.

ğŸ› ï¸ **Built entirely from scratch** â€” from mechanical assembly and hardware wiring to computer vision pipeline and control logic â€” as part of a Reinforcement Learning & Robotics course capstone.

----
### ğŸ¯ Key Features

| Feature                           | Description                                                                                                 |
| --------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| **Real-time Gesture Recognition** | Detects hand landmarks using MediaPipe and classifies gestures using a deep CNN trained on custom datasets. |
| **Robotic Motor Control**         | Commands dual DC motors using PWM signals through an H-Bridge interface on a Raspberry Pi.                  |
| **Vision-Based Command Mapping**  | Gesture â†’ Action mapping enables autonomous motion based purely on visual inputs.                           |
| **End-to-End Integration**        | Combines computer vision, deep learning, embedded control, and robotic hardware.                            |
| **Built-from-Scratch Robot**      | Complete mechanical build, wiring, and code integration designed by the team.                               |

----
### ğŸ§© System Architecture

```
graph TD
A[Camera Input (OpenCV)] --> B[Hand Landmark Detection (MediaPipe)]
B --> C[Gesture Classification (CNN Model - TensorFlow)]
C --> D[Gesture-to-Action Mapping Layer]
D --> E[Motor Control Unit (PWM + GPIO)]
E --> F[Robot Movement (H-Bridge + DC Motors)]
```

----

### âš™ï¸ Technical Stack

| Domain                    | Technologies                                                                                  |
| ------------------------- | --------------------------------------------------------------------------------------------- |
| **Computer Vision**       | OpenCV, MediaPipe, Contour Analysis, Background Subtraction                                   |
| **Deep Learning**         | TensorFlow, Keras (CNN Gesture Classifier)                                                    |
| **Embedded Systems**      | Raspberry Pi 4, MDD10A H-Bridge Motor Driver                                                  |
| **Programming Languages** | Python                                                                                        |
| **Hardware Control**      | RPi.GPIO for PWM and direction control                                                        |
| **Dataset**               | Custom hand gesture dataset (`ok`, `stop`, `thumbs up`, `peace`, etc.) collected for training |

----

### ğŸš€ Project Pipeline

1. **Data Acquisition**

- Collected gesture videos under varying lighting/backgrounds.

- Labeled gestures for classification model training.

2. **Model Training**

- Built and trained a CNN using TensorFlow to classify gestures.

- Optimized using transfer learning for limited dataset size.

3. **Computer Vision Processing**

- Used MediaPipe Hands for landmark extraction.

- Integrated OpenCV for preprocessing and segmentation.

4. **Gesture â†’ Action Mapping**

- Mapped recognized gestures to robot control commands:

- âœ‹ Stop â†’ Halt motors

- ğŸ‘ Move Forward

- ğŸ‘ Move Backward

- ğŸ¤Ÿ Turn Right

- âœŒï¸ Turn Left

5. **Motor Control**

- Controlled motors via GPIO PWM signals through MDD10A H-Bridge.

- Implemented smooth speed control and safe shutdown routines.

----
### ğŸ“¸ Real Robot Build

ğŸ§© Fully built and tested â€” below are snapshots from our final build and live testing sessions.

- âœ… Physical robot assembled using custom chassis, motor mount, and onboard camera.

- âœ… Controlled entirely via visual input â€” no manual remote or wired control.
  
----

### ğŸ§ª Experimental Results

| Gesture      | Model Accuracy | Latency (ms) | Action       |
| ------------ | -------------- | ------------ | ------------ |
| ğŸ‘ Thumbs Up | 98.2%          | 43           | Move Forward |
| âœ‹ Stop       | 99.1%          | 39           | Halt         |
| âœŒï¸ Peace     | 97.3%          | 41           | Turn Left    |
| ğŸ¤Ÿ Rock      | 96.7%          | 45           | Turn Right   |


Overall System FPS: ~20 FPS on Raspberry Pi 4
Inference Latency: <50ms (real-time gesture recognition)

----
### ğŸ§° Hardware Used

| Component                         | Purpose                                     |
| --------------------------------- | ------------------------------------------- |
| **Raspberry Pi 4 (4GB)**          | Primary compute unit for vision and control |
| **Pi Camera Module**              | Real-time image capture                     |
| **MDD10A Dual Motor Driver**      | PWM-based control of DC motors              |
| **DC Motors x2**                  | Left and right wheel drive                  |
| **Custom Chassis + Battery Pack** | Robot structure and power supply            |

---

### ğŸª„ Example Demonstration



